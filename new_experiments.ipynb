{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import get_sentiment_detection_dataset, get_sentiment_classification_dataset\n",
    "from models import ALMBert, BasicBert\n",
    "from trainer import Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Bert model experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect sentimen detection and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 1.461\t metrics: <function accuracy at 0x7f032008ba30>: 0.2707\t \n",
      " Validation loss: 1.331\t metrics: <function accuracy at 0x7f032008ba30>: 0.4201\t \n",
      "Epoch: 1\t Train loss: 1.364\t metrics: <function accuracy at 0x7f032008ba30>: 0.3552\t \n",
      " Validation loss: 1.223\t metrics: <function accuracy at 0x7f032008ba30>: 0.5712\t \n",
      "Epoch: 2\t Train loss: 1.29\t metrics: <function accuracy at 0x7f032008ba30>: 0.4217\t \n",
      " Validation loss: 1.17\t metrics: <function accuracy at 0x7f032008ba30>: 0.6172\t \n",
      "Epoch: 3\t Train loss: 1.23\t metrics: <function accuracy at 0x7f032008ba30>: 0.4758\t \n",
      " Validation loss: 1.122\t metrics: <function accuracy at 0x7f032008ba30>: 0.6406\t \n",
      "Epoch: 4\t Train loss: 1.189\t metrics: <function accuracy at 0x7f032008ba30>: 0.5243\t \n",
      " Validation loss: 1.078\t metrics: <function accuracy at 0x7f032008ba30>: 0.6519\t \n",
      "Epoch: 5\t Train loss: 1.15\t metrics: <function accuracy at 0x7f032008ba30>: 0.5551\t \n",
      " Validation loss: 1.053\t metrics: <function accuracy at 0x7f032008ba30>: 0.6536\t \n",
      "Epoch: 6\t Train loss: 1.126\t metrics: <function accuracy at 0x7f032008ba30>: 0.5683\t \n",
      " Validation loss: 1.044\t metrics: <function accuracy at 0x7f032008ba30>: 0.6484\t \n",
      "Epoch: 7\t Train loss: 1.095\t metrics: <function accuracy at 0x7f032008ba30>: 0.5936\t \n",
      " Validation loss: 1.023\t metrics: <function accuracy at 0x7f032008ba30>: 0.6441\t \n",
      "Epoch: 8\t Train loss: 1.078\t metrics: <function accuracy at 0x7f032008ba30>: 0.6019\t \n",
      " Validation loss: 1.001\t metrics: <function accuracy at 0x7f032008ba30>: 0.6476\t \n",
      "Epoch: 9\t Train loss: 1.062\t metrics: <function accuracy at 0x7f032008ba30>: 0.6072\t \n",
      " Validation loss: 1.0\t metrics: <function accuracy at 0x7f032008ba30>: 0.6467\t \n",
      "Epoch: 10\t Train loss: 1.048\t metrics: <function accuracy at 0x7f032008ba30>: 0.6128\t \n",
      " Validation loss: 0.9836\t metrics: <function accuracy at 0x7f032008ba30>: 0.6484\t \n",
      "Epoch: 11\t Train loss: 1.039\t metrics: <function accuracy at 0x7f032008ba30>: 0.6132\t \n",
      " Validation loss: 0.9789\t metrics: <function accuracy at 0x7f032008ba30>: 0.6458\t \n",
      "Epoch: 12\t Train loss: 1.011\t metrics: <function accuracy at 0x7f032008ba30>: 0.617\t \n",
      " Validation loss: 0.9696\t metrics: <function accuracy at 0x7f032008ba30>: 0.645\t \n",
      "Epoch: 13\t Train loss: 1.009\t metrics: <function accuracy at 0x7f032008ba30>: 0.6197\t \n",
      " Validation loss: 0.9626\t metrics: <function accuracy at 0x7f032008ba30>: 0.6424\t \n",
      "Epoch: 14\t Train loss: 0.9992\t metrics: <function accuracy at 0x7f032008ba30>: 0.6203\t \n",
      " Validation loss: 0.9612\t metrics: <function accuracy at 0x7f032008ba30>: 0.638\t \n",
      "Epoch: 15\t Train loss: 0.9858\t metrics: <function accuracy at 0x7f032008ba30>: 0.6225\t \n",
      " Validation loss: 0.9559\t metrics: <function accuracy at 0x7f032008ba30>: 0.6345\t \n",
      "Epoch: 16\t Train loss: 0.972\t metrics: <function accuracy at 0x7f032008ba30>: 0.6205\t \n",
      " Validation loss: 0.9433\t metrics: <function accuracy at 0x7f032008ba30>: 0.6372\t \n",
      "Epoch: 17\t Train loss: 0.963\t metrics: <function accuracy at 0x7f032008ba30>: 0.619\t \n",
      " Validation loss: 0.9425\t metrics: <function accuracy at 0x7f032008ba30>: 0.6354\t \n",
      "Epoch: 18\t Train loss: 0.9471\t metrics: <function accuracy at 0x7f032008ba30>: 0.6246\t \n",
      " Validation loss: 0.9325\t metrics: <function accuracy at 0x7f032008ba30>: 0.6302\t \n",
      "Epoch: 19\t Train loss: 0.9362\t metrics: <function accuracy at 0x7f032008ba30>: 0.6283\t \n",
      " Validation loss: 0.9267\t metrics: <function accuracy at 0x7f032008ba30>: 0.6319\t \n",
      "Epoch: 20\t Train loss: 0.921\t metrics: <function accuracy at 0x7f032008ba30>: 0.6261\t \n",
      " Validation loss: 0.9208\t metrics: <function accuracy at 0x7f032008ba30>: 0.6311\t \n",
      "Epoch: 21\t Train loss: 0.9156\t metrics: <function accuracy at 0x7f032008ba30>: 0.6281\t \n",
      " Validation loss: 0.9151\t metrics: <function accuracy at 0x7f032008ba30>: 0.6276\t \n",
      "Epoch: 22\t Train loss: 0.8986\t metrics: <function accuracy at 0x7f032008ba30>: 0.6314\t \n",
      " Validation loss: 0.9157\t metrics: <function accuracy at 0x7f032008ba30>: 0.6181\t \n",
      "Epoch: 23\t Train loss: 0.8952\t metrics: <function accuracy at 0x7f032008ba30>: 0.6283\t \n",
      " Validation loss: 0.9075\t metrics: <function accuracy at 0x7f032008ba30>: 0.6311\t \n",
      "Epoch: 24\t Train loss: 0.8882\t metrics: <function accuracy at 0x7f032008ba30>: 0.625\t \n",
      " Validation loss: 0.9129\t metrics: <function accuracy at 0x7f032008ba30>: 0.6172\t \n",
      "Epoch: 25\t Train loss: 0.871\t metrics: <function accuracy at 0x7f032008ba30>: 0.6372\t \n",
      " Validation loss: 0.9112\t metrics: <function accuracy at 0x7f032008ba30>: 0.6259\t \n",
      "Epoch: 26\t Train loss: 0.8707\t metrics: <function accuracy at 0x7f032008ba30>: 0.6334\t \n",
      " Validation loss: 0.9117\t metrics: <function accuracy at 0x7f032008ba30>: 0.625\t \n",
      "Epoch: 27\t Train loss: 0.8631\t metrics: <function accuracy at 0x7f032008ba30>: 0.6305\t \n",
      " Validation loss: 0.9139\t metrics: <function accuracy at 0x7f032008ba30>: 0.6146\t \n",
      "Epoch: 28\t Train loss: 0.8535\t metrics: <function accuracy at 0x7f032008ba30>: 0.637\t \n",
      " Validation loss: 0.9233\t metrics: <function accuracy at 0x7f032008ba30>: 0.6016\t \n",
      "Epoch: 29\t Train loss: 0.841\t metrics: <function accuracy at 0x7f032008ba30>: 0.6359\t \n",
      " Validation loss: 0.918\t metrics: <function accuracy at 0x7f032008ba30>: 0.6042\t \n",
      "Epoch: 30\t Train loss: 0.8306\t metrics: <function accuracy at 0x7f032008ba30>: 0.6392\t \n",
      " Validation loss: 0.9214\t metrics: <function accuracy at 0x7f032008ba30>: 0.599\t \n",
      "Test loss: 0.9703\t metrics: <function accuracy at 0x7f032008ba30>: 0.5887\t\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_sentiment_classification_dataset(5000)\n",
    "\n",
    "BATCH = 128\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH, drop_last=True, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_ds, batch_size=BATCH, drop_last=True, shuffle=False)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH, drop_last=False, shuffle=False)\n",
    "\n",
    "LR = 0.0001\n",
    "\n",
    "model = BasicBert(4, 0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.functional.cross_entropy\n",
    "metrics = [\n",
    "    torchmetrics.functional.accuracy,\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=criterion,\n",
    "    verbose=True,\n",
    "    metrics=metrics,\n",
    "    train_dataloader=train_dl,\n",
    "    valid_dataloader=valid_dl,\n",
    "    test_dataloader=test_dl\n",
    ")\n",
    "\n",
    "trainer.train(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect's sentimen classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 1.048\t metrics: <function accuracy at 0x7f032008ba30>: 0.4537\t \n",
      " Validation loss: 1.004\t metrics: <function accuracy at 0x7f032008ba30>: 0.5312\t \n",
      "Epoch: 1\t Train loss: 0.9484\t metrics: <function accuracy at 0x7f032008ba30>: 0.5396\t \n",
      " Validation loss: 0.949\t metrics: <function accuracy at 0x7f032008ba30>: 0.5312\t \n",
      "Epoch: 2\t Train loss: 0.9176\t metrics: <function accuracy at 0x7f032008ba30>: 0.5446\t \n",
      " Validation loss: 0.9039\t metrics: <function accuracy at 0x7f032008ba30>: 0.5625\t \n",
      "Epoch: 3\t Train loss: 0.8749\t metrics: <function accuracy at 0x7f032008ba30>: 0.5904\t \n",
      " Validation loss: 0.8554\t metrics: <function accuracy at 0x7f032008ba30>: 0.7383\t \n",
      "Epoch: 4\t Train loss: 0.8267\t metrics: <function accuracy at 0x7f032008ba30>: 0.6434\t \n",
      " Validation loss: 0.7865\t metrics: <function accuracy at 0x7f032008ba30>: 0.7266\t \n",
      "Epoch: 5\t Train loss: 0.769\t metrics: <function accuracy at 0x7f032008ba30>: 0.6925\t \n",
      " Validation loss: 0.7087\t metrics: <function accuracy at 0x7f032008ba30>: 0.7383\t \n",
      "Epoch: 6\t Train loss: 0.6844\t metrics: <function accuracy at 0x7f032008ba30>: 0.7344\t \n",
      " Validation loss: 0.6426\t metrics: <function accuracy at 0x7f032008ba30>: 0.7656\t \n",
      "Epoch: 7\t Train loss: 0.6284\t metrics: <function accuracy at 0x7f032008ba30>: 0.764\t \n",
      " Validation loss: 0.5983\t metrics: <function accuracy at 0x7f032008ba30>: 0.7812\t \n",
      "Epoch: 8\t Train loss: 0.5837\t metrics: <function accuracy at 0x7f032008ba30>: 0.7907\t \n",
      " Validation loss: 0.5698\t metrics: <function accuracy at 0x7f032008ba30>: 0.793\t \n",
      "Epoch: 9\t Train loss: 0.5373\t metrics: <function accuracy at 0x7f032008ba30>: 0.8125\t \n",
      " Validation loss: 0.5668\t metrics: <function accuracy at 0x7f032008ba30>: 0.8008\t \n",
      "Epoch: 10\t Train loss: 0.5045\t metrics: <function accuracy at 0x7f032008ba30>: 0.822\t \n",
      " Validation loss: 0.5457\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 11\t Train loss: 0.4775\t metrics: <function accuracy at 0x7f032008ba30>: 0.8348\t \n",
      " Validation loss: 0.5588\t metrics: <function accuracy at 0x7f032008ba30>: 0.8164\t \n",
      "Epoch: 12\t Train loss: 0.4582\t metrics: <function accuracy at 0x7f032008ba30>: 0.8449\t \n",
      " Validation loss: 0.5304\t metrics: <function accuracy at 0x7f032008ba30>: 0.832\t \n",
      "Epoch: 13\t Train loss: 0.4227\t metrics: <function accuracy at 0x7f032008ba30>: 0.8549\t \n",
      " Validation loss: 0.5037\t metrics: <function accuracy at 0x7f032008ba30>: 0.8281\t \n",
      "Epoch: 14\t Train loss: 0.3951\t metrics: <function accuracy at 0x7f032008ba30>: 0.8677\t \n",
      " Validation loss: 0.5201\t metrics: <function accuracy at 0x7f032008ba30>: 0.8359\t \n",
      "Epoch: 15\t Train loss: 0.3925\t metrics: <function accuracy at 0x7f032008ba30>: 0.8633\t \n",
      " Validation loss: 0.5403\t metrics: <function accuracy at 0x7f032008ba30>: 0.8242\t \n",
      "Epoch: 16\t Train loss: 0.3646\t metrics: <function accuracy at 0x7f032008ba30>: 0.8767\t \n",
      " Validation loss: 0.5343\t metrics: <function accuracy at 0x7f032008ba30>: 0.8242\t \n",
      "Epoch: 17\t Train loss: 0.3575\t metrics: <function accuracy at 0x7f032008ba30>: 0.8811\t \n",
      " Validation loss: 0.5515\t metrics: <function accuracy at 0x7f032008ba30>: 0.8281\t \n",
      "Epoch: 18\t Train loss: 0.3348\t metrics: <function accuracy at 0x7f032008ba30>: 0.889\t \n",
      " Validation loss: 0.5475\t metrics: <function accuracy at 0x7f032008ba30>: 0.8281\t \n",
      "Epoch: 19\t Train loss: 0.3037\t metrics: <function accuracy at 0x7f032008ba30>: 0.8923\t \n",
      " Validation loss: 0.6032\t metrics: <function accuracy at 0x7f032008ba30>: 0.8242\t \n",
      "Epoch: 20\t Train loss: 0.3074\t metrics: <function accuracy at 0x7f032008ba30>: 0.8951\t \n",
      " Validation loss: 0.6542\t metrics: <function accuracy at 0x7f032008ba30>: 0.7969\t \n",
      "Epoch: 21\t Train loss: 0.2968\t metrics: <function accuracy at 0x7f032008ba30>: 0.8934\t \n",
      " Validation loss: 0.6063\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 22\t Train loss: 0.2951\t metrics: <function accuracy at 0x7f032008ba30>: 0.8945\t \n",
      " Validation loss: 0.6144\t metrics: <function accuracy at 0x7f032008ba30>: 0.8242\t \n",
      "Epoch: 23\t Train loss: 0.2921\t metrics: <function accuracy at 0x7f032008ba30>: 0.9023\t \n",
      " Validation loss: 0.6072\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 24\t Train loss: 0.2654\t metrics: <function accuracy at 0x7f032008ba30>: 0.9023\t \n",
      " Validation loss: 0.6122\t metrics: <function accuracy at 0x7f032008ba30>: 0.832\t \n",
      "Epoch: 25\t Train loss: 0.2538\t metrics: <function accuracy at 0x7f032008ba30>: 0.9152\t \n",
      " Validation loss: 0.599\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 26\t Train loss: 0.2546\t metrics: <function accuracy at 0x7f032008ba30>: 0.9102\t \n",
      " Validation loss: 0.6143\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 27\t Train loss: 0.2629\t metrics: <function accuracy at 0x7f032008ba30>: 0.9113\t \n",
      " Validation loss: 0.6764\t metrics: <function accuracy at 0x7f032008ba30>: 0.8047\t \n",
      "Epoch: 28\t Train loss: 0.2541\t metrics: <function accuracy at 0x7f032008ba30>: 0.9102\t \n",
      " Validation loss: 0.6541\t metrics: <function accuracy at 0x7f032008ba30>: 0.8008\t \n",
      "Epoch: 29\t Train loss: 0.236\t metrics: <function accuracy at 0x7f032008ba30>: 0.9163\t \n",
      " Validation loss: 0.6875\t metrics: <function accuracy at 0x7f032008ba30>: 0.8164\t \n",
      "Epoch: 30\t Train loss: 0.2213\t metrics: <function accuracy at 0x7f032008ba30>: 0.9208\t \n",
      " Validation loss: 0.6192\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 31\t Train loss: 0.2202\t metrics: <function accuracy at 0x7f032008ba30>: 0.9191\t \n",
      " Validation loss: 0.6076\t metrics: <function accuracy at 0x7f032008ba30>: 0.8242\t \n",
      "Test loss: 0.5881\t metrics: <function accuracy at 0x7f032008ba30>: 0.8227\t\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_sentiment_classification_dataset(0)\n",
    "\n",
    "BATCH = 256\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH, drop_last=True, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_ds, batch_size=BATCH, drop_last=True, shuffle=False)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH, drop_last=False, shuffle=False)\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "model = BasicBert(3, 0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.functional.cross_entropy\n",
    "metrics = [\n",
    "    torchmetrics.functional.accuracy,\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=criterion,\n",
    "    verbose=True,\n",
    "    metrics=metrics,\n",
    "    train_dataloader=train_dl,\n",
    "    valid_dataloader=valid_dl,\n",
    "    test_dataloader=test_dl\n",
    ")\n",
    "\n",
    "trainer.train(60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect sentimen detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 0.6688\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6197\t <function recall at 0x7f79289e88b0>: 0.6197\t <function precision at 0x7f79289e8700>: 0.6197\t \n",
      " Validation loss: 0.6426\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.658\t <function recall at 0x7f79289e88b0>: 0.658\t <function precision at 0x7f79289e8700>: 0.658\t \n",
      "Epoch: 1\t Train loss: 0.6526\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6368\t <function recall at 0x7f79289e88b0>: 0.6368\t <function precision at 0x7f79289e8700>: 0.6368\t \n",
      " Validation loss: 0.6414\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6458\t <function recall at 0x7f79289e88b0>: 0.6458\t <function precision at 0x7f79289e8700>: 0.6458\t \n",
      "Epoch: 2\t Train loss: 0.6446\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6448\t <function recall at 0x7f79289e88b0>: 0.6448\t <function precision at 0x7f79289e8700>: 0.6448\t \n",
      " Validation loss: 0.643\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6415\t <function recall at 0x7f79289e88b0>: 0.6415\t <function precision at 0x7f79289e8700>: 0.6415\t \n",
      "Epoch: 3\t Train loss: 0.6375\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6504\t <function recall at 0x7f79289e88b0>: 0.6504\t <function precision at 0x7f79289e8700>: 0.6504\t \n",
      " Validation loss: 0.6462\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6502\t <function recall at 0x7f79289e88b0>: 0.6502\t <function precision at 0x7f79289e8700>: 0.6502\t \n",
      "Epoch: 4\t Train loss: 0.6358\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6541\t <function recall at 0x7f79289e88b0>: 0.6541\t <function precision at 0x7f79289e8700>: 0.6541\t \n",
      " Validation loss: 0.6523\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.6424\t <function recall at 0x7f79289e88b0>: 0.6424\t <function precision at 0x7f79289e8700>: 0.6424\t \n",
      "Test loss: 0.6769\t metrics: <function accuracy at 0x7f79289a3ac0>: 0.5946\t<function recall at 0x7f79289e88b0>: 0.5946\t<function precision at 0x7f79289e8700>: 0.5946\t\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_sentiment_detection_dataset(5000)\n",
    "\n",
    "BATCH = 64\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH, drop_last=True, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_ds, batch_size=BATCH, drop_last=True, shuffle=False)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH, drop_last=False, shuffle=False)\n",
    "\n",
    "model = BasicBert(2, 0.6)\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.functional.cross_entropy\n",
    "metrics = [\n",
    "    torchmetrics.functional.accuracy,\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=criterion,\n",
    "    verbose=True,\n",
    "    metrics=metrics,\n",
    "    train_dataloader=train_dl,\n",
    "    valid_dataloader=valid_dl,\n",
    "    test_dataloader=test_dl\n",
    ")\n",
    "\n",
    "trainer.train(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALM-Bert model experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect sentimen detection and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 1.392\t metrics: <function accuracy at 0x7f032008ba30>: 0.2162\t \n",
      " Validation loss: 1.36\t metrics: <function accuracy at 0x7f032008ba30>: 0.1338\t \n",
      "Epoch: 1\t Train loss: 1.379\t metrics: <function accuracy at 0x7f032008ba30>: 0.2742\t \n",
      " Validation loss: 1.329\t metrics: <function accuracy at 0x7f032008ba30>: 0.6064\t \n",
      "Epoch: 2\t Train loss: 1.352\t metrics: <function accuracy at 0x7f032008ba30>: 0.3742\t \n",
      " Validation loss: 1.281\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 3\t Train loss: 1.32\t metrics: <function accuracy at 0x7f032008ba30>: 0.4689\t \n",
      " Validation loss: 1.234\t metrics: <function accuracy at 0x7f032008ba30>: 0.6523\t \n",
      "Epoch: 4\t Train loss: 1.288\t metrics: <function accuracy at 0x7f032008ba30>: 0.5322\t \n",
      " Validation loss: 1.207\t metrics: <function accuracy at 0x7f032008ba30>: 0.6523\t \n",
      "Epoch: 5\t Train loss: 1.268\t metrics: <function accuracy at 0x7f032008ba30>: 0.551\t \n",
      " Validation loss: 1.184\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 6\t Train loss: 1.242\t metrics: <function accuracy at 0x7f032008ba30>: 0.5664\t \n",
      " Validation loss: 1.161\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 7\t Train loss: 1.219\t metrics: <function accuracy at 0x7f032008ba30>: 0.5809\t \n",
      " Validation loss: 1.141\t metrics: <function accuracy at 0x7f032008ba30>: 0.6523\t \n",
      "Epoch: 8\t Train loss: 1.19\t metrics: <function accuracy at 0x7f032008ba30>: 0.5881\t \n",
      " Validation loss: 1.108\t metrics: <function accuracy at 0x7f032008ba30>: 0.6533\t \n",
      "Epoch: 9\t Train loss: 1.167\t metrics: <function accuracy at 0x7f032008ba30>: 0.6051\t \n",
      " Validation loss: 1.082\t metrics: <function accuracy at 0x7f032008ba30>: 0.6533\t \n",
      "Epoch: 10\t Train loss: 1.139\t metrics: <function accuracy at 0x7f032008ba30>: 0.6096\t \n",
      " Validation loss: 1.065\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 11\t Train loss: 1.115\t metrics: <function accuracy at 0x7f032008ba30>: 0.6146\t \n",
      " Validation loss: 1.031\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 12\t Train loss: 1.097\t metrics: <function accuracy at 0x7f032008ba30>: 0.6191\t \n",
      " Validation loss: 1.006\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 13\t Train loss: 1.077\t metrics: <function accuracy at 0x7f032008ba30>: 0.6182\t \n",
      " Validation loss: 0.9941\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 14\t Train loss: 1.059\t metrics: <function accuracy at 0x7f032008ba30>: 0.6193\t \n",
      " Validation loss: 0.9716\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 15\t Train loss: 1.048\t metrics: <function accuracy at 0x7f032008ba30>: 0.6201\t \n",
      " Validation loss: 0.9641\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 16\t Train loss: 1.035\t metrics: <function accuracy at 0x7f032008ba30>: 0.6246\t \n",
      " Validation loss: 0.9575\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 17\t Train loss: 1.021\t metrics: <function accuracy at 0x7f032008ba30>: 0.6291\t \n",
      " Validation loss: 0.944\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 18\t Train loss: 1.015\t metrics: <function accuracy at 0x7f032008ba30>: 0.623\t \n",
      " Validation loss: 0.9239\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 19\t Train loss: 0.9954\t metrics: <function accuracy at 0x7f032008ba30>: 0.6256\t \n",
      " Validation loss: 0.9183\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 20\t Train loss: 0.984\t metrics: <function accuracy at 0x7f032008ba30>: 0.6227\t \n",
      " Validation loss: 0.9146\t metrics: <function accuracy at 0x7f032008ba30>: 0.6523\t \n",
      "Epoch: 21\t Train loss: 0.9783\t metrics: <function accuracy at 0x7f032008ba30>: 0.625\t \n",
      " Validation loss: 0.9042\t metrics: <function accuracy at 0x7f032008ba30>: 0.6523\t \n",
      "Epoch: 22\t Train loss: 0.9675\t metrics: <function accuracy at 0x7f032008ba30>: 0.6338\t \n",
      " Validation loss: 0.8998\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 23\t Train loss: 0.9584\t metrics: <function accuracy at 0x7f032008ba30>: 0.6291\t \n",
      " Validation loss: 0.8915\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 24\t Train loss: 0.9448\t metrics: <function accuracy at 0x7f032008ba30>: 0.6338\t \n",
      " Validation loss: 0.8921\t metrics: <function accuracy at 0x7f032008ba30>: 0.6533\t \n",
      "Epoch: 25\t Train loss: 0.9364\t metrics: <function accuracy at 0x7f032008ba30>: 0.6281\t \n",
      " Validation loss: 0.8869\t metrics: <function accuracy at 0x7f032008ba30>: 0.6504\t \n",
      "Epoch: 26\t Train loss: 0.9321\t metrics: <function accuracy at 0x7f032008ba30>: 0.6305\t \n",
      " Validation loss: 0.883\t metrics: <function accuracy at 0x7f032008ba30>: 0.6533\t \n",
      "Epoch: 27\t Train loss: 0.9243\t metrics: <function accuracy at 0x7f032008ba30>: 0.6324\t \n",
      " Validation loss: 0.8752\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 28\t Train loss: 0.9221\t metrics: <function accuracy at 0x7f032008ba30>: 0.634\t \n",
      " Validation loss: 0.8726\t metrics: <function accuracy at 0x7f032008ba30>: 0.6543\t \n",
      "Epoch: 29\t Train loss: 0.911\t metrics: <function accuracy at 0x7f032008ba30>: 0.6326\t \n",
      " Validation loss: 0.8699\t metrics: <function accuracy at 0x7f032008ba30>: 0.6514\t \n",
      "Epoch: 30\t Train loss: 0.9062\t metrics: <function accuracy at 0x7f032008ba30>: 0.6301\t \n",
      " Validation loss: 0.873\t metrics: <function accuracy at 0x7f032008ba30>: 0.6553\t \n",
      "Epoch: 31\t Train loss: 0.903\t metrics: <function accuracy at 0x7f032008ba30>: 0.6287\t \n",
      " Validation loss: 0.8632\t metrics: <function accuracy at 0x7f032008ba30>: 0.6455\t \n",
      "Test loss: 0.9113\t metrics: <function accuracy at 0x7f032008ba30>: 0.6039\t\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_sentiment_classification_dataset(5000)\n",
    "\n",
    "BATCH = 512\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH, drop_last=True, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_ds, batch_size=BATCH, drop_last=True, shuffle=False)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH, drop_last=False, shuffle=False)\n",
    "\n",
    "LR = 0.0001\n",
    "\n",
    "model = ALMBert(4, 0.8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.functional.cross_entropy\n",
    "metrics = [\n",
    "    torchmetrics.functional.accuracy,\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=criterion,\n",
    "    verbose=True,\n",
    "    metrics=metrics,\n",
    "    train_dataloader=train_dl,\n",
    "    valid_dataloader=valid_dl,\n",
    "    test_dataloader=test_dl\n",
    ")\n",
    "\n",
    "trainer.train(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect's sentimen classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 1.035\t metrics: <function accuracy at 0x7f032008ba30>: 0.4794\t \n",
      " Validation loss: 1.051\t metrics: <function accuracy at 0x7f032008ba30>: 0.6328\t \n",
      "Epoch: 1\t Train loss: 0.858\t metrics: <function accuracy at 0x7f032008ba30>: 0.7148\t \n",
      " Validation loss: 0.9907\t metrics: <function accuracy at 0x7f032008ba30>: 0.5039\t \n",
      "Epoch: 2\t Train loss: 0.7123\t metrics: <function accuracy at 0x7f032008ba30>: 0.7567\t \n",
      " Validation loss: 0.8985\t metrics: <function accuracy at 0x7f032008ba30>: 0.5547\t \n",
      "Epoch: 3\t Train loss: 0.5999\t metrics: <function accuracy at 0x7f032008ba30>: 0.8103\t \n",
      " Validation loss: 0.5996\t metrics: <function accuracy at 0x7f032008ba30>: 0.793\t \n",
      "Epoch: 4\t Train loss: 0.516\t metrics: <function accuracy at 0x7f032008ba30>: 0.8337\t \n",
      " Validation loss: 0.5646\t metrics: <function accuracy at 0x7f032008ba30>: 0.7969\t \n",
      "Epoch: 5\t Train loss: 0.4645\t metrics: <function accuracy at 0x7f032008ba30>: 0.8504\t \n",
      " Validation loss: 0.6365\t metrics: <function accuracy at 0x7f032008ba30>: 0.7891\t \n",
      "Epoch: 6\t Train loss: 0.4382\t metrics: <function accuracy at 0x7f032008ba30>: 0.8516\t \n",
      " Validation loss: 0.5867\t metrics: <function accuracy at 0x7f032008ba30>: 0.8008\t \n",
      "Epoch: 7\t Train loss: 0.4091\t metrics: <function accuracy at 0x7f032008ba30>: 0.8616\t \n",
      " Validation loss: 0.638\t metrics: <function accuracy at 0x7f032008ba30>: 0.8008\t \n",
      "Epoch: 8\t Train loss: 0.3822\t metrics: <function accuracy at 0x7f032008ba30>: 0.8627\t \n",
      " Validation loss: 0.5646\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 9\t Train loss: 0.3521\t metrics: <function accuracy at 0x7f032008ba30>: 0.8778\t \n",
      " Validation loss: 0.6465\t metrics: <function accuracy at 0x7f032008ba30>: 0.8086\t \n",
      "Epoch: 10\t Train loss: 0.3339\t metrics: <function accuracy at 0x7f032008ba30>: 0.8856\t \n",
      " Validation loss: 0.6068\t metrics: <function accuracy at 0x7f032008ba30>: 0.8086\t \n",
      "Epoch: 11\t Train loss: 0.3192\t metrics: <function accuracy at 0x7f032008ba30>: 0.8834\t \n",
      " Validation loss: 0.5134\t metrics: <function accuracy at 0x7f032008ba30>: 0.8398\t \n",
      "Epoch: 12\t Train loss: 0.3112\t metrics: <function accuracy at 0x7f032008ba30>: 0.8934\t \n",
      " Validation loss: 0.5545\t metrics: <function accuracy at 0x7f032008ba30>: 0.8164\t \n",
      "Epoch: 13\t Train loss: 0.2863\t metrics: <function accuracy at 0x7f032008ba30>: 0.9001\t \n",
      " Validation loss: 0.6326\t metrics: <function accuracy at 0x7f032008ba30>: 0.8086\t \n",
      "Epoch: 14\t Train loss: 0.2905\t metrics: <function accuracy at 0x7f032008ba30>: 0.8984\t \n",
      " Validation loss: 0.7256\t metrics: <function accuracy at 0x7f032008ba30>: 0.7891\t \n",
      "Epoch: 15\t Train loss: 0.2623\t metrics: <function accuracy at 0x7f032008ba30>: 0.9135\t \n",
      " Validation loss: 0.5745\t metrics: <function accuracy at 0x7f032008ba30>: 0.8359\t \n",
      "Epoch: 16\t Train loss: 0.2469\t metrics: <function accuracy at 0x7f032008ba30>: 0.9118\t \n",
      " Validation loss: 0.7139\t metrics: <function accuracy at 0x7f032008ba30>: 0.8203\t \n",
      "Epoch: 17\t Train loss: 0.2482\t metrics: <function accuracy at 0x7f032008ba30>: 0.9102\t \n",
      " Validation loss: 0.6162\t metrics: <function accuracy at 0x7f032008ba30>: 0.8477\t \n",
      "Test loss: 0.5917\t metrics: <function accuracy at 0x7f032008ba30>: 0.8222\t\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_sentiment_classification_dataset(0)\n",
    "\n",
    "BATCH = 256\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH, drop_last=True, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_ds, batch_size=BATCH, drop_last=True, shuffle=False)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH, drop_last=False, shuffle=False)\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "model = ALMBert(3, 0.60)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.functional.cross_entropy\n",
    "metrics = [\n",
    "    torchmetrics.functional.accuracy,\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=criterion,\n",
    "    verbose=True,\n",
    "    metrics=metrics,\n",
    "    train_dataloader=train_dl,\n",
    "    valid_dataloader=valid_dl,\n",
    "    test_dataloader=test_dl\n",
    ")\n",
    "\n",
    "trainer.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.model, \"./trained/ALMBert_sentiment_classification.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect sentimen detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 0.6946\t metrics: <function accuracy at 0x7f032008ba30>: 0.5176\t \n",
      " Validation loss: 0.6634\t metrics: <function accuracy at 0x7f032008ba30>: 0.6285\t \n",
      "Epoch: 1\t Train loss: 0.6628\t metrics: <function accuracy at 0x7f032008ba30>: 0.6303\t \n",
      " Validation loss: 0.6383\t metrics: <function accuracy at 0x7f032008ba30>: 0.658\t \n",
      "Epoch: 2\t Train loss: 0.6493\t metrics: <function accuracy at 0x7f032008ba30>: 0.639\t \n",
      " Validation loss: 0.6333\t metrics: <function accuracy at 0x7f032008ba30>: 0.6684\t \n",
      "Epoch: 3\t Train loss: 0.6422\t metrics: <function accuracy at 0x7f032008ba30>: 0.6484\t \n",
      " Validation loss: 0.6372\t metrics: <function accuracy at 0x7f032008ba30>: 0.6502\t \n",
      "Epoch: 4\t Train loss: 0.6346\t metrics: <function accuracy at 0x7f032008ba30>: 0.6475\t \n",
      " Validation loss: 0.6475\t metrics: <function accuracy at 0x7f032008ba30>: 0.638\t \n",
      "Epoch: 5\t Train loss: 0.6306\t metrics: <function accuracy at 0x7f032008ba30>: 0.6552\t \n",
      " Validation loss: 0.6373\t metrics: <function accuracy at 0x7f032008ba30>: 0.6675\t \n",
      "Epoch: 6\t Train loss: 0.6235\t metrics: <function accuracy at 0x7f032008ba30>: 0.6608\t \n",
      " Validation loss: 0.6549\t metrics: <function accuracy at 0x7f032008ba30>: 0.6345\t \n",
      "Epoch: 7\t Train loss: 0.6186\t metrics: <function accuracy at 0x7f032008ba30>: 0.6612\t \n",
      " Validation loss: 0.6501\t metrics: <function accuracy at 0x7f032008ba30>: 0.6406\t \n",
      "Epoch: 8\t Train loss: 0.6124\t metrics: <function accuracy at 0x7f032008ba30>: 0.6679\t \n",
      " Validation loss: 0.6685\t metrics: <function accuracy at 0x7f032008ba30>: 0.6233\t \n",
      "Epoch: 9\t Train loss: 0.6111\t metrics: <function accuracy at 0x7f032008ba30>: 0.6697\t \n",
      " Validation loss: 0.6626\t metrics: <function accuracy at 0x7f032008ba30>: 0.6432\t \n",
      "Epoch: 10\t Train loss: 0.6015\t metrics: <function accuracy at 0x7f032008ba30>: 0.6799\t \n",
      " Validation loss: 0.6881\t metrics: <function accuracy at 0x7f032008ba30>: 0.5833\t \n",
      "Epoch: 11\t Train loss: 0.6025\t metrics: <function accuracy at 0x7f032008ba30>: 0.6777\t \n",
      " Validation loss: 0.673\t metrics: <function accuracy at 0x7f032008ba30>: 0.6363\t \n",
      "Epoch: 12\t Train loss: 0.5944\t metrics: <function accuracy at 0x7f032008ba30>: 0.6802\t \n",
      " Validation loss: 0.6881\t metrics: <function accuracy at 0x7f032008ba30>: 0.6155\t \n",
      "Epoch: 13\t Train loss: 0.5906\t metrics: <function accuracy at 0x7f032008ba30>: 0.6797\t \n",
      " Validation loss: 0.6876\t metrics: <function accuracy at 0x7f032008ba30>: 0.6224\t \n",
      "Epoch: 14\t Train loss: 0.5858\t metrics: <function accuracy at 0x7f032008ba30>: 0.6875\t \n",
      " Validation loss: 0.7014\t metrics: <function accuracy at 0x7f032008ba30>: 0.6033\t \n",
      "Epoch: 15\t Train loss: 0.5815\t metrics: <function accuracy at 0x7f032008ba30>: 0.6871\t \n",
      " Validation loss: 0.7224\t metrics: <function accuracy at 0x7f032008ba30>: 0.5747\t \n",
      "Epoch: 16\t Train loss: 0.5754\t metrics: <function accuracy at 0x7f032008ba30>: 0.6957\t \n",
      " Validation loss: 0.7277\t metrics: <function accuracy at 0x7f032008ba30>: 0.5747\t \n",
      "Epoch: 17\t Train loss: 0.5722\t metrics: <function accuracy at 0x7f032008ba30>: 0.6948\t \n",
      " Validation loss: 0.7334\t metrics: <function accuracy at 0x7f032008ba30>: 0.5851\t \n",
      "Epoch: 18\t Train loss: 0.5683\t metrics: <function accuracy at 0x7f032008ba30>: 0.6975\t \n",
      " Validation loss: 0.7509\t metrics: <function accuracy at 0x7f032008ba30>: 0.5729\t \n",
      "Epoch: 19\t Train loss: 0.568\t metrics: <function accuracy at 0x7f032008ba30>: 0.6931\t \n",
      " Validation loss: 0.7648\t metrics: <function accuracy at 0x7f032008ba30>: 0.559\t \n",
      "Epoch: 20\t Train loss: 0.5631\t metrics: <function accuracy at 0x7f032008ba30>: 0.7011\t \n",
      " Validation loss: 0.7596\t metrics: <function accuracy at 0x7f032008ba30>: 0.5625\t \n",
      "Epoch: 21\t Train loss: 0.5569\t metrics: <function accuracy at 0x7f032008ba30>: 0.7044\t \n",
      " Validation loss: 0.7718\t metrics: <function accuracy at 0x7f032008ba30>: 0.5616\t \n",
      "Epoch: 22\t Train loss: 0.5594\t metrics: <function accuracy at 0x7f032008ba30>: 0.7026\t \n",
      " Validation loss: 0.775\t metrics: <function accuracy at 0x7f032008ba30>: 0.5512\t \n",
      "Epoch: 23\t Train loss: 0.5516\t metrics: <function accuracy at 0x7f032008ba30>: 0.7113\t \n",
      " Validation loss: 0.7977\t metrics: <function accuracy at 0x7f032008ba30>: 0.5443\t \n",
      "Epoch: 24\t Train loss: 0.5473\t metrics: <function accuracy at 0x7f032008ba30>: 0.712\t \n",
      " Validation loss: 0.8166\t metrics: <function accuracy at 0x7f032008ba30>: 0.5321\t \n",
      "Epoch: 25\t Train loss: 0.5487\t metrics: <function accuracy at 0x7f032008ba30>: 0.7015\t \n",
      " Validation loss: 0.8034\t metrics: <function accuracy at 0x7f032008ba30>: 0.5573\t \n",
      "Test loss: 0.8321\t metrics: <function accuracy at 0x7f032008ba30>: 0.5133\t\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_sentiment_detection_dataset(5000)\n",
    "\n",
    "BATCH = 128\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH, drop_last=True, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_ds, batch_size=BATCH, drop_last=True, shuffle=False)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH, drop_last=False, shuffle=False)\n",
    "\n",
    "LR = 0.0001\n",
    "\n",
    "model = ALMBert(2, 0.6)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.functional.cross_entropy\n",
    "metrics = [\n",
    "    torchmetrics.functional.accuracy,\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=criterion,\n",
    "    verbose=True,\n",
    "    metrics=metrics,\n",
    "    train_dataloader=train_dl,\n",
    "    valid_dataloader=valid_dl,\n",
    "    test_dataloader=test_dl\n",
    ")\n",
    "\n",
    "trainer.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pawel/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 38\u001b[0m\n\u001b[1;32m     32\u001b[0m labels \u001b[39m=\u001b[39m {\n\u001b[1;32m     33\u001b[0m     \u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[39m2\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 38\u001b[0m     preds \u001b[39m=\u001b[39m model(\n\u001b[1;32m     39\u001b[0m         review_encoding[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     40\u001b[0m         review_encoding[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     41\u001b[0m         aspect_encoding[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     42\u001b[0m         aspect_encoding[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(preds)\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(labels[torch\u001b[39m.\u001b[39margmax(preds)])\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/studia/NLP/ABSA-LAPTOPS/models/alm_bert.py:31\u001b[0m, in \u001b[0;36mALMBert.forward\u001b[0;34m(self, text_ids, text_attention, aspect_ids, aspect_attention)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, text_ids, text_attention, aspect_ids, aspect_attention):\n\u001b[0;32m---> 31\u001b[0m   context, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m     32\u001b[0m     input_ids\u001b[39m=\u001b[39;49mtext_ids,\n\u001b[1;32m     33\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mtext_attention,\n\u001b[1;32m     34\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m   )\n\u001b[1;32m     36\u001b[0m   aspect, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[1;32m     37\u001b[0m     input_ids\u001b[39m=\u001b[39maspect_ids,\n\u001b[1;32m     38\u001b[0m     attention_mask\u001b[39m=\u001b[39maspect_attention,\n\u001b[1;32m     39\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   )\n\u001b[1;32m     41\u001b[0m   asp_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39masp_att(context, aspect, aspect)\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1017\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1018\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1020\u001b[0m )\n\u001b[1;32m   1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1022\u001b[0m     embedding_output,\n\u001b[1;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1032\u001b[0m )\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:231\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    228\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    232\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    234\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from models import ALMBert\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "review = \"This laptop is bad\"\n",
    "aspect = \"laptop general\"\n",
    "\n",
    "review_encoding = tokenizer.encode_plus(\n",
    "  review,\n",
    "  add_special_tokens=True,\n",
    "  max_length=75,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "aspect_encoding = tokenizer.encode_plus(\n",
    "  review,\n",
    "  add_special_tokens=True,\n",
    "  max_length=4,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "labels = {\n",
    "    0: \"positive\",\n",
    "    1: \"negative\",\n",
    "    2: \"neutral\",\n",
    "}\n",
    "with torch.no_grad():\n",
    "    preds = model(\n",
    "        review_encoding['input_ids'],\n",
    "        review_encoding['attention_mask'],\n",
    "        aspect_encoding['input_ids'],\n",
    "        aspect_encoding['attention_mask'],\n",
    "    )\n",
    "print(preds)\n",
    "print(labels[torch.argmax(preds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a38050f8ca24262ead381cbc68a5f26ee5ce1e0f01d0472f124efe95ebd2613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
